{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7239393,"sourceType":"datasetVersion","datasetId":4192794}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install tensorflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-19T17:59:46.670028Z","iopub.execute_input":"2023-12-19T17:59:46.670631Z","iopub.status.idle":"2023-12-19T17:59:57.610636Z","shell.execute_reply.started":"2023-12-19T17:59:46.670591Z","shell.execute_reply":"2023-12-19T17:59:57.608981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install nltk","metadata":{"execution":{"iopub.status.busy":"2023-12-19T17:59:57.614004Z","iopub.execute_input":"2023-12-19T17:59:57.614971Z","iopub.status.idle":"2023-12-19T18:00:00.341532Z","shell.execute_reply.started":"2023-12-19T17:59:57.614814Z","shell.execute_reply":"2023-12-19T18:00:00.340176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-19T18:00:00.343028Z","iopub.execute_input":"2023-12-19T18:00:00.343405Z","iopub.status.idle":"2023-12-19T18:00:00.350514Z","shell.execute_reply.started":"2023-12-19T18:00:00.343374Z","shell.execute_reply":"2023-12-19T18:00:00.349069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom nltk.corpus import stopwords\nimport os\nimport nltk\nimport re\nfrom nltk.stem import ISRIStemmer\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-12-19T18:00:00.353873Z","iopub.execute_input":"2023-12-19T18:00:00.354272Z","iopub.status.idle":"2023-12-19T18:00:00.368156Z","shell.execute_reply.started":"2023-12-19T18:00:00.354240Z","shell.execute_reply":"2023-12-19T18:00:00.366922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/dataset/test _no_label.csv\")\ntrain = pd.read_excel(\"/kaggle/input/dataset/train.xlsx\")\n\n\ntrain.columns = ['text','sentiment']\ntrain_text = train['text']\n\n# print(train)\n# print(test)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T18:00:00.369798Z","iopub.execute_input":"2023-12-19T18:00:00.370210Z","iopub.status.idle":"2023-12-19T18:00:04.195237Z","shell.execute_reply.started":"2023-12-19T18:00:00.370177Z","shell.execute_reply":"2023-12-19T18:00:04.193934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemm\n!pip install qalsadi","metadata":{"execution":{"iopub.status.busy":"2023-12-19T18:00:04.197056Z","iopub.execute_input":"2023-12-19T18:00:04.197549Z","iopub.status.idle":"2023-12-19T18:00:15.988049Z","shell.execute_reply.started":"2023-12-19T18:00:04.197504Z","shell.execute_reply":"2023-12-19T18:00:15.986406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nimport qalsadi.lemmatizer\n\n# Create an instance of the Lemmatizer\nlemer = qalsadi.lemmatizer.Lemmatizer()\n\ndef solve(word_list):\n    arabic_reg = re.compile('[\\u0600-\\u06FF]+')\n    arabic_words = [word for word in word_list if arabic_reg.fullmatch(word)]\n    lemmatized_words = [lemer.lemmatize_text(word) for word in arabic_words]\n    return lemmatized_words\n\ndef remove_and_lemmatize(tokenized_corpus):\n    filtered_corpus = []\n    for tokenized_document in tokenized_corpus:\n        filtered_document = solve(tokenized_document)\n        filtered_corpus.append(filtered_document)\n    return filtered_corpus","metadata":{"execution":{"iopub.status.busy":"2023-12-19T18:00:15.995840Z","iopub.execute_input":"2023-12-19T18:00:15.996376Z","iopub.status.idle":"2023-12-19T18:00:16.831373Z","shell.execute_reply.started":"2023-12-19T18:00:15.996326Z","shell.execute_reply":"2023-12-19T18:00:16.830465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the text\ntokenized_train = [word_tokenize(sentence) for sentence in train_text]\n\n# Remove stop words and lemmatize\nstop_words_arabic = set(stopwords.words('arabic'))\nfiltered_text_train = [\n    [word for word in sentence if word not in stop_words_arabic]\n    for sentence in tokenized_train\n]\n\nresulttrain1 = remove_and_lemmatize(filtered_text_train)\n# print(resulttrain1[0])\nresulttrain=[]\nfor row in resulttrain1:\n  x = []\n  for lemma in row:\n    for c in lemma:\n      x.append(c)\n  resulttrain.append(x)\n\n# print(resulttrain)\n\nresult_train_df = pd.DataFrame({\n    'review_description':resulttrain\n})\n\n# print(result_train_df)\n\ncv = CountVectorizer()\ntodoc = [\" \".join(doc) for doc in resulttrain]\nx = cv.fit_transform(todoc)\nfeature_names = cv.get_feature_names_out()\nxarr = x.toarray()\ndf = pd.DataFrame(xarr, columns=feature_names)\n\ndftrain = pd.DataFrame({\n    'review_description': resulttrain,\n    'rating':train['sentiment']\n})\n\n# print(dftrain.head)\n\ntokenized_test = [word_tokenize(sentence) for sentence in test['review_description']]\n\nfiltered_texttest = [\n    [word for word in sentence if word not in stop_words_arabic]\n    for sentence in tokenized_test\n]\n\nresulttest1=remove_and_lemmatize(filtered_texttest)\n\nresulttest=[]\nfor row in resulttest1:\n  x = []\n  for lemma in row:\n    for c in lemma:\n      x.append(c)\n  resulttest.append(x)\n\ndftest = pd.DataFrame({\n    'ID': test['ID'],\n    'review_description': resulttest\n})\n# print(resulttest)\n\nfinal=[]\nfor i in resulttest:\n  sentcount=[]\n  for j in feature_names:\n    count = i.count(j)\n    sentcount.append(count)\n  final.append(sentcount)\n\n\n\ndftestcount = pd.DataFrame({\n    'review_description':final\n})\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T18:00:16.832939Z","iopub.execute_input":"2023-12-19T18:00:16.834218Z","iopub.status.idle":"2023-12-19T18:06:39.224411Z","shell.execute_reply.started":"2023-12-19T18:00:16.834173Z","shell.execute_reply":"2023-12-19T18:06:39.222816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# Activation functions and their derivatives\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef initialize_parameters(input_size, hidden_size, output_size):\n    Wxh = np.random.randn(input_size, hidden_size)\n    Whh = np.random.randn(hidden_size, hidden_size)\n    Why = np.random.randn(hidden_size, output_size)\n    bh = np.zeros((1, hidden_size))\n    by = np.zeros((1, output_size))\n    return Wxh, Whh, Why, bh, by\n\ndef rnn_forward(inputs, h_prev, Wxh, Whh, Why, bh, by):\n    h_t = sigmoid(np.dot(inputs, Wxh) + np.dot(h_prev, Whh) + bh)\n    y_pred = sigmoid(np.dot(h_t, Why) + by)\n    return h_t, y_pred\n\ndef rnn_backward(inputs, h_prev, h_t, y_pred, target, Wxh, Whh, Why, bh, by, learning_rate):\n    dy = y_pred - target\n    dWhy = np.dot(h_t.T, dy)\n    dby = np.sum(dy, axis=0, keepdims=True)\n    dh = np.dot(dy, Why.T) * h_t * (1 - h_t)\n    dWhh = np.dot(h_prev.T, dh)\n    dWxh = np.dot(inputs.T, dh)\n    dbh = np.sum(dh, axis=0, keepdims=True)\n\n    Wxh -= learning_rate * dWxh\n    Whh -= learning_rate * dWhh\n    Why -= learning_rate * dWhy\n    bh -= learning_rate * dbh\n    by -= learning_rate * dby\n\n    return Wxh, Whh, Why, bh, by\n\ndef train_rnn(X, y, input_size, hidden_size, output_size, learning_rate, num_epochs):\n    Wxh, Whh, Why, bh, by = initialize_parameters(input_size, hidden_size, output_size)\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        h_prev = np.zeros((1, hidden_size))\n\n        for inputs, target in zip(X, y):\n            inputs = np.array([inputs])  # Convert inputs to a 2D array\n            h_t, y_pred = rnn_forward(inputs, h_prev, Wxh, Whh, Why, bh, by)\n            Wxh, Whh, Why, bh, by = rnn_backward(inputs, h_prev, h_t, y_pred, target, Wxh, Whh, Why, bh, by, learning_rate)\n\n            if(y_pred<0.35):\n              y_pred=-1\n            elif(y_pred<0.65):\n              y_pred=0\n            else:\n              y_pred=1\n            #print(y_pred,target)\n            loss = 0.5 * (y_pred - target)**2  # Mean squared error loss\n            total_loss += loss\n\n            h_prev = h_t\n\n        average_loss = total_loss / len(y)\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}')\n\n    return Wxh, Whh, Why, bh, by\n\n\ndef test_rnn(X_test, Wxh, Whh, Why, bh, by):\n    cnt = 0\n    h_prev = np.zeros((1, Wxh.shape[1]))\n    predictions=[]\n    for inputs in X_test:\n        inputs = np.array([inputs])  # Convert inputs to a 2D array\n        h_t, y_pred = rnn_forward(inputs, h_prev, Wxh, Whh, Why, bh, by)\n\n        # Post-process y_pred based on your logic\n        if y_pred < 0.35:\n            y_pred = -1\n        elif y_pred < 0.65:\n            y_pred = 0\n        else:\n            y_pred = 1\n\n        predictions.append(y_pred)\n        h_prev = h_t\n\n    return predictions\n\n# Example usage:\nX_train = np.array(df)\ny_train = np.array(dftrain[\"rating\"])\ninput_size = len(df.columns)\nhidden_size = 20\noutput_size = 1\nlearning_rate = 0.01\nnum_epochs = 30\n\n# Train the RNN and get the weights\ntrained_weights = train_rnn(X_train, y_train, input_size, hidden_size, output_size, learning_rate, num_epochs)\n\n# Test the RNN using the trained weights\nX_test = dftestcount['review_description']\n\npredictions = test_rnn(X_test, *trained_weights)\nfinal_df_test = pd.DataFrame({\n    'ID': dftest['ID'],\n    'review_description': dftest['review_description'],\n    'rating_test': predictions\n})\nprint(final_df_test)\nfinal_df_test.to_csv('/kaggle/working/output.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-19T18:13:29.166600Z","iopub.execute_input":"2023-12-19T18:13:29.167001Z","iopub.status.idle":"2023-12-19T19:07:59.237424Z","shell.execute_reply.started":"2023-12-19T18:13:29.166971Z","shell.execute_reply":"2023-12-19T19:07:59.236136Z"},"trusted":true},"execution_count":null,"outputs":[]}]}