{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8Ql905rOmU8"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "Jx6n841YPQCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "W1ZQnooeQEOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import ISRIStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "dyKn_WenQBRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"/kaggle/input/dataset/test _no_label.csv\")\n",
        "train = pd.read_excel(\"/kaggle/input/dataset/train.xlsx\")\n",
        "\n",
        "\n",
        "train.columns = ['text','sentiment']\n",
        "train_text = train['text']\n",
        "\n",
        "# print(train)\n",
        "# print(test)"
      ],
      "metadata": {
        "id": "sdVyqQ1YP-l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemm\n",
        "!pip install qalsadi"
      ],
      "metadata": {
        "id": "-ZSQd8SmP7UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import qalsadi.lemmatizer\n",
        "\n",
        "# Create an instance of the Lemmatizer\n",
        "lemer = qalsadi.lemmatizer.Lemmatizer()\n",
        "\n",
        "def solve(word_list):\n",
        "    arabic_reg = re.compile('[\\u0600-\\u06FF]+')\n",
        "    arabic_words = [word for word in word_list if arabic_reg.fullmatch(word)]\n",
        "    lemmatized_words = [lemer.lemmatize_text(word) for word in arabic_words]\n",
        "    return lemmatized_words\n",
        "\n",
        "def remove_and_lemmatize(tokenized_corpus):\n",
        "    filtered_corpus = []\n",
        "    for tokenized_document in tokenized_corpus:\n",
        "        filtered_document = solve(tokenized_document)\n",
        "        filtered_corpus.append(filtered_document)\n",
        "    return filtered_corpus"
      ],
      "metadata": {
        "id": "YwFJjQ06PUHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenized_train = [word_tokenize(sentence) for sentence in train_text]\n",
        "\n",
        "# Remove stop words and lemmatize\n",
        "stop_words_arabic = set(stopwords.words('arabic'))\n",
        "filtered_text_train = [\n",
        "    [word for word in sentence if word not in stop_words_arabic]\n",
        "    for sentence in tokenized_train\n",
        "]\n",
        "\n",
        "resulttrain1 = remove_and_lemmatize(filtered_text_train)\n",
        "# print(resulttrain1[0])\n",
        "resulttrain=[]\n",
        "for row in resulttrain1:\n",
        "  x = []\n",
        "  for lemma in row:\n",
        "    for c in lemma:\n",
        "      x.append(c)\n",
        "  resulttrain.append(x)\n",
        "\n",
        "# print(resulttrain)\n",
        "\n",
        "result_train_df = pd.DataFrame({\n",
        "    'review_description':resulttrain\n",
        "})\n",
        "\n",
        "# print(result_train_df)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "todoc = [\" \".join(doc) for doc in resulttrain]\n",
        "x = cv.fit_transform(todoc)\n",
        "feature_names = cv.get_feature_names_out()\n",
        "xarr = x.toarray()\n",
        "df = pd.DataFrame(xarr, columns=feature_names)\n",
        "\n",
        "dftrain = pd.DataFrame({\n",
        "    'review_description': resulttrain,\n",
        "    'rating':train['sentiment']\n",
        "})\n",
        "\n",
        "# print(dftrain.head)\n",
        "\n",
        "tokenized_test = [word_tokenize(sentence) for sentence in test['review_description']]\n",
        "\n",
        "filtered_texttest = [\n",
        "    [word for word in sentence if word not in stop_words_arabic]\n",
        "    for sentence in tokenized_test\n",
        "]\n",
        "\n",
        "resulttest1=remove_and_lemmatize(filtered_texttest)\n",
        "\n",
        "resulttest=[]\n",
        "for row in resulttest1:\n",
        "  x = []\n",
        "  for lemma in row:\n",
        "    for c in lemma:\n",
        "      x.append(c)\n",
        "  resulttest.append(x)\n",
        "\n",
        "dftest = pd.DataFrame({\n",
        "    'ID': test['ID'],\n",
        "    'review_description': resulttest\n",
        "})\n",
        "# print(resulttest)\n",
        "\n",
        "final=[]\n",
        "for i in resulttest:\n",
        "  sentcount=[]\n",
        "  for j in feature_names:\n",
        "    count = i.count(j)\n",
        "    sentcount.append(count)\n",
        "  final.append(sentcount)\n",
        "\n",
        "\n",
        "\n",
        "dftestcount = pd.DataFrame({\n",
        "    'review_description':final\n",
        "})"
      ],
      "metadata": {
        "id": "QneW7-4APWbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}