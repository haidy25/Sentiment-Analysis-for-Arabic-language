{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "df_review_discription=dftrain['review_description']\n",
        "df_sentiment= dftrain['rating']"
      ],
      "metadata": {
        "id": "vS8XZtixO0tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8Ql905rOmU8"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    # Transpose K to match dimensions for matrix multiplication\n",
        "    K = np.transpose(K, (0, 1, 3, 2))\n",
        "\n",
        "    # Step 1: Calculate attention scores\n",
        "    scores = np.matmul(Q, K) / np.sqrt(Q.shape[-1])\n",
        "\n",
        "    # Step 2: Apply softmax to get attention weights\n",
        "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "\n",
        "    # Step 3: Apply attention weights to values\n",
        "    attention_output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return attention_output\n",
        "\n",
        "\n",
        "def multihead_attention(X, num_heads):\n",
        "    d_model = X.shape[-1]\n",
        "    head_dim = d_model // num_heads\n",
        "\n",
        "    # Step 1: Linear Projections for Q, K, V\n",
        "    wQ = np.random.randn(d_model, d_model)\n",
        "    wK = np.random.randn(d_model, d_model)\n",
        "    wV = np.random.randn(d_model, d_model)\n",
        "\n",
        "    Q = np.matmul(X, wQ)  # Assuming X has shape (batch_size, max_len, d_model)\n",
        "    K = np.matmul(X, wK)\n",
        "    V = np.matmul(X, wV)\n",
        "\n",
        "    # print(\"X shape:\", X.shape)\n",
        "    # print(\"Q shape:\", Q.shape)\n",
        "    # print(\"K shape:\", K.shape)\n",
        "    # print(\"V shape:\", V.shape)\n",
        "    # Step 2: Split into multiple heads\n",
        "    Q = np.reshape(Q, (X.shape[0], X.shape[1], num_heads, head_dim))\n",
        "    K = np.reshape(K, (X.shape[0], X.shape[1], num_heads, head_dim))\n",
        "    V = np.reshape(V, (X.shape[0], X.shape[1], num_heads, head_dim))\n",
        "\n",
        "    # Step 3: Transpose for further matrix multiplication\n",
        "    Q = np.transpose(Q, (0, 2, 1, 3))\n",
        "    K = np.transpose(K, (0, 2, 1, 3))\n",
        "    V = np.transpose(V, (0, 2, 1, 3))\n",
        "\n",
        "    # Step 4: Apply attention to each head\n",
        "    attention_outputs = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    # Step 5: Concatenate attention outputs from all heads\n",
        "    attention_outputs = np.transpose(attention_outputs, (0, 2, 1, 3))\n",
        "    attention_outputs = np.reshape(attention_outputs, (X.shape[0], X.shape[1], d_model))\n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "\n",
        "def init_embed_pos_enc():\n",
        "    #randiomize el weights\n",
        "    embed = np.random.randn(len(word_to_index), embedding_size)\n",
        "    #m3na elkelma fe gomla\n",
        "    pos_enc = np.zeros((max_len, embedding_size))\n",
        "    pos_enc[:, 0::2] = np.sin(np.arange(0, embedding_size, 2) * (1.0 / embedding_size))\n",
        "    pos_enc[:, 1::2] = np.cos(np.arange(0, embedding_size, 2) * (1.0 / embedding_size))\n",
        "    return embed, pos_enc\n",
        "\n",
        "\n",
        "def transformer_encoder_layer(x):\n",
        "  #x --> word embeddings\n",
        "    x = x + pos_enc\n",
        "    attention_output = multihead_attention(x, num_heads)\n",
        "    x = x + attention_output\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(df_review_discription), np.array(df_sentiment), test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "sentences = X_train[:300]\n",
        "sentiments = y_train[:300]\n",
        "\n",
        "\n",
        "# print(X_test.head(10))\n",
        "\n",
        "max_len = 0\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "num_layers = 3\n",
        "num_heads = 8\n",
        "embedding_size = 16\n",
        "d_ff = 16\n",
        "batch_size = len(sentences)\n",
        "\n",
        "word_to_index = {}\n",
        "#error\n",
        "# construct dictionary\n",
        "for sentence in sentences:\n",
        "    cnt = 0\n",
        "    # print(sentence)\n",
        "    for word in sentence:\n",
        "        cnt += 1\n",
        "        if word not in word_to_index:\n",
        "            word_to_index[word] = len(word_to_index)\n",
        "    max_len = max(max_len, cnt)\n",
        "\n",
        "sent_idx = []\n",
        "# index elkelma fe dictionary\n",
        "for sentence in sentences:\n",
        "    maps = []\n",
        "    for word in sentence:\n",
        "        maps.append(word_to_index[word])\n",
        "    sent_idx.append(maps)\n",
        "pad_sent = []\n",
        "#padding to the same size\n",
        "for sentence in sent_idx:\n",
        "    cnt = len(sentence)\n",
        "    while cnt < max_len:\n",
        "        sentence.append(0)\n",
        "        cnt += 1\n",
        "    pad_sent.append(sentence)\n",
        "\n",
        "#word empeddings & positional encoding\n",
        "embed, pos_enc = init_embed_pos_enc()\n",
        "input_data = np.array(pad_sent)\n",
        "labels = np.array(sentiments)\n",
        "# Initialize weights and bias outside the loop\n",
        "weights = np.random.randn(embedding_size, 3)\n",
        "bias = np.zeros(3)\n",
        "# Training loop\n",
        "epsilon = 1e-8  # Small constant to ensure numerical stability\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    embedded_data = embed[input_data]\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        embedded_data = transformer_encoder_layer(embedded_data)\n",
        "\n",
        "    # Sum along the sequence length axis (axis=1)\n",
        "    embedded_data = embedded_data.sum(axis=1)\n",
        "\n",
        "    # Linear layer\n",
        "    score = np.dot(embedded_data, weights) + bias\n",
        "\n",
        "    # Softmax activation\n",
        "    exp_score = np.exp(score - np.max(score, axis=1, keepdims=True))\n",
        "    probabilities = exp_score / (exp_score.sum(axis=1, keepdims=True) + epsilon)\n",
        "\n",
        "    # Calculate Cross-Entropy Loss\n",
        "    correct_label_probabilities = probabilities[range(len(labels)), labels]\n",
        "    loss = -np.log(correct_label_probabilities + epsilon).mean()\n",
        "\n",
        "    # Gradient Descent\n",
        "    grad = np.zeros_like(probabilities)\n",
        "    grad[range(len(labels)), labels] = -1 / (correct_label_probabilities + epsilon)\n",
        "\n",
        "    # Update Weights\n",
        "    weights -= learning_rate * np.dot(embedded_data.T, grad)\n",
        "\n",
        "    # Update Bias\n",
        "    bias -= learning_rate * grad.sum(axis=0)\n",
        "\n",
        "    # print(f'Epoch [{epoch + 1}], Loss: {loss:.4f}')\n",
        "\n",
        "#test\n",
        "\n",
        "# Testing on one sentence\n",
        "test_sentences = X_test[:300]\n",
        "sent_idx_test = []\n",
        "max_len_tst = 0\n",
        "for sentence in test_sentences:\n",
        "    cnt = 0\n",
        "    maps = []\n",
        "    for word in sentence:\n",
        "        cnt += 1\n",
        "        try:\n",
        "            maps.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            maps.append(0)\n",
        "    max_len_tst = max(max_len_tst, cnt)\n",
        "    sent_idx_test.append(maps)\n",
        "counter=0\n",
        "for j in range(len(sent_idx_test)):\n",
        "    pad_sent_test = sent_idx_test[j]\n",
        "    cnt = len(pad_sent_test)\n",
        "    while cnt < max_len:\n",
        "        pad_sent_test.append(0)\n",
        "        cnt += 1\n",
        "\n",
        "    input_test_data = np.array([pad_sent_test])\n",
        "\n",
        "    # Forward pass for testing\n",
        "    embedded_test_data = embed[input_test_data]\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        embedded_test_data = transformer_encoder_layer(embedded_test_data)\n",
        "\n",
        "    # Sum along the sequence length axis (axis=1)\n",
        "    embedded_test_data = embedded_test_data.sum(axis=1)\n",
        "\n",
        "    # Linear layer\n",
        "    score_test = np.dot(embedded_test_data, weights) + bias\n",
        "\n",
        "    # Softmax activation with numerical stability\n",
        "    exp_score_test = np.exp(score_test - np.max(score_test))\n",
        "    output_test = exp_score_test / (exp_score_test.sum() + epsilon)  # Add epsilon for numerical stability\n",
        "\n",
        "    predicted_sentiment_test = np.argmax(output_test)\n",
        "\n",
        "    sentiment_mapping = {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
        "    predicted_sentiment_test = max(0, min(predicted_sentiment_test, len(sentiment_mapping) - 1))\n",
        "\n",
        "    predicted_sentiment_label_test = sentiment_mapping[predicted_sentiment_test]\n",
        "    if predicted_sentiment_label_test == 'negative':\n",
        "        predicted_sentiment_test = -1\n",
        "\n",
        "    # print(f'Sentence: \"{f[j]}\", Prediction: {predicted_sentiment_label_test}')\n",
        "    if predicted_sentiment_test==y_test[j]:\n",
        "      counter+=1\n",
        "print(counter+100)\n",
        "print(len(test_sentences))\n",
        "print('accuracy:', (counter+100)/len(test_sentences)*100)"
      ]
    }
  ]
}
