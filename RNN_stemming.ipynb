{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSMQt_YBi8ak"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2fAdQ6-jU5C"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnzBIRSJgoiA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import ISRIStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar0rgjYsgrqG"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"/content/test _no_label.csv\")\n",
        "train = pd.read_excel(\"/content/train.xlsx\")\n",
        "\n",
        "\n",
        "train.columns = ['text','sentiment']\n",
        "train_text = train['text']\n",
        "\n",
        "# print(train)\n",
        "# print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpDx6StXgsUl"
      },
      "outputs": [],
      "source": [
        "stemmer = ISRIStemmer()\n",
        "def solve(word_list):\n",
        "    arabic_reg = re.compile('[\\u0600-\\u06FF]+')\n",
        "    arabic_words = [word for word in word_list if arabic_reg.fullmatch(word)]\n",
        "    stemmed_words = [stemmer.stem(word) for word in arabic_words]\n",
        "    return stemmed_words\n",
        "\n",
        "def remove_and_stemm(tokenized_corpus):\n",
        "    filtered_corpus = []\n",
        "    for tokenized_document in tokenized_corpus:\n",
        "        filtered_document = solve(tokenized_document)\n",
        "        filtered_corpus.append(filtered_document)\n",
        "    return filtered_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKjENo0SgsLu"
      },
      "outputs": [],
      "source": [
        "tokenized_train = [word_tokenize(sentence) for sentence in train_text]\n",
        "\n",
        "stop_words_arabic = set(stopwords.words('arabic'))\n",
        "filtered_texttrain = [\n",
        "    [word for word in sentence if word not in stop_words_arabic]\n",
        "    for sentence in tokenized_train\n",
        "]\n",
        "\n",
        "resulttrain=remove_and_stemm(filtered_texttrain)\n",
        "\n",
        "cv=CountVectorizer()\n",
        "todoc = [\" \".join(doc) for doc in resulttrain]\n",
        "x=cv.fit_transform(todoc)\n",
        "feature_names = cv.get_feature_names_out()\n",
        "xarr= x.toarray()\n",
        "df = pd.DataFrame(xarr, columns=feature_names)\n",
        "\n",
        "dftrain = pd.DataFrame({\n",
        "    'review_description': resulttrain,\n",
        "    'rating':train['sentiment']\n",
        "})\n",
        "\n",
        "\n",
        "tokenized_test = [word_tokenize(sentence) for sentence in test['review_description']]\n",
        "\n",
        "filtered_texttest = [\n",
        "    [word for word in sentence if word not in stop_words_arabic]\n",
        "    for sentence in tokenized_test\n",
        "]\n",
        "\n",
        "resulttest=remove_and_stemm(filtered_texttest)\n",
        "dftest = pd.DataFrame({\n",
        "    'ID': test['ID'],\n",
        "    'review_description': resulttest\n",
        "})\n",
        "\n",
        "\n",
        "final=[]\n",
        "for i in resulttest:\n",
        "  sentcount=[]\n",
        "  for j in feature_names:\n",
        "    count = i.count(j)\n",
        "    sentcount.append(count)\n",
        "  final.append(sentcount)\n",
        "\n",
        "\n",
        "dftestcount = pd.DataFrame({\n",
        "    'review_description':final\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrPpjnfiysH5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Activation functions and their derivatives\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def initialize_parameters(input_size, hidden_size, output_size):\n",
        "    Wxh = np.random.randn(input_size, hidden_size)\n",
        "    Whh = np.random.randn(hidden_size, hidden_size)\n",
        "    Why = np.random.randn(hidden_size, output_size)\n",
        "    bh = np.zeros((1, hidden_size))\n",
        "    by = np.zeros((1, output_size))\n",
        "    return Wxh, Whh, Why, bh, by\n",
        "\n",
        "def rnn_forward(inputs, h_prev, Wxh, Whh, Why, bh, by):\n",
        "    h_t = sigmoid(np.dot(inputs, Wxh) + np.dot(h_prev, Whh) + bh)\n",
        "    y_pred = sigmoid(np.dot(h_t, Why) + by)\n",
        "    return h_t, y_pred\n",
        "\n",
        "def rnn_backward(inputs, h_prev, h_t, y_pred, target, Wxh, Whh, Why, bh, by, learning_rate):\n",
        "    dy = y_pred - target\n",
        "    dWhy = np.dot(h_t.T, dy)\n",
        "    dby = np.sum(dy, axis=0, keepdims=True)\n",
        "    dh = np.dot(dy, Why.T) * h_t * (1 - h_t)\n",
        "    dWhh = np.dot(h_prev.T, dh)\n",
        "    dWxh = np.dot(inputs.T, dh)\n",
        "    dbh = np.sum(dh, axis=0, keepdims=True)\n",
        "\n",
        "    Wxh -= learning_rate * dWxh\n",
        "    Whh -= learning_rate * dWhh\n",
        "    Why -= learning_rate * dWhy\n",
        "    bh -= learning_rate * dbh\n",
        "    by -= learning_rate * dby\n",
        "\n",
        "    return Wxh, Whh, Why, bh, by\n",
        "\n",
        "def train_rnn(X, y, input_size, hidden_size, output_size, learning_rate, num_epochs):\n",
        "    Wxh, Whh, Why, bh, by = initialize_parameters(input_size, hidden_size, output_size)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        h_prev = np.zeros((1, hidden_size))\n",
        "\n",
        "        for inputs, target in zip(X, y):\n",
        "            inputs = np.array([inputs])  # Convert inputs to a 2D array\n",
        "            h_t, y_pred = rnn_forward(inputs, h_prev, Wxh, Whh, Why, bh, by)\n",
        "            Wxh, Whh, Why, bh, by = rnn_backward(inputs, h_prev, h_t, y_pred, target, Wxh, Whh, Why, bh, by, learning_rate)\n",
        "\n",
        "            if(y_pred<0.35):\n",
        "              y_pred=-1\n",
        "            elif(y_pred<0.65):\n",
        "              y_pred=0\n",
        "            else:\n",
        "              y_pred=1\n",
        "            #print(y_pred,target)\n",
        "            loss = 0.5 * (y_pred - target)**2  # Mean squared error loss\n",
        "            total_loss += loss\n",
        "\n",
        "            h_prev = h_t\n",
        "\n",
        "        average_loss = total_loss / len(y)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}')\n",
        "\n",
        "    return Wxh, Whh, Why, bh, by\n",
        "\n",
        "\n",
        "def test_rnn(X_test, Wxh, Whh, Why, bh, by):\n",
        "    cnt = 0\n",
        "    h_prev = np.zeros((1, Wxh.shape[1]))\n",
        "    predictions=[]\n",
        "    for inputs in X_test:\n",
        "        inputs = np.array([inputs])  # Convert inputs to a 2D array\n",
        "        h_t, y_pred = rnn_forward(inputs, h_prev, Wxh, Whh, Why, bh, by)\n",
        "\n",
        "        # Post-process y_pred based on your logic\n",
        "        if y_pred < 0.35:\n",
        "            y_pred = -1\n",
        "        elif y_pred < 0.65:\n",
        "            y_pred = 0\n",
        "        else:\n",
        "            y_pred = 1\n",
        "\n",
        "        predictions.append(y_pred)\n",
        "        h_prev = h_t\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(np.array(df), np.array(dftrain[\"rating\"]), test_size=0.2, random_state=42)\n",
        "X_train = np.array(df)\n",
        "y_train = np.array(dftrain[\"rating\"])\n",
        "input_size = len(df.columns)\n",
        "hidden_size = 20\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "num_epochs =50\n",
        "\n",
        "trained_weights = train_rnn(X_train, y_train, input_size, hidden_size, output_size, learning_rate, num_epochs)\n",
        "X_test = dftestcount['review_description']\n",
        "\n",
        "print(trained_weights)\n",
        "\n",
        "predictions = test_rnn(X_test, *trained_weights)\n",
        "final_df_test = pd.DataFrame({\n",
        "    'ID': dftest['ID'],\n",
        "    'review_description': dftest['review_description'],\n",
        "    'rating_test': predictions\n",
        "})\n",
        "\n",
        "print(final_df_test)\n",
        "final_df_test.to_csv('output_stemming.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_review_discription=dftrain['review_description']\n",
        "df_sentiment= dftrain['rating']\n",
        "# df_review_discription.head(5)\n",
        "# df_sentiment.head(5)"
      ],
      "metadata": {
        "id": "k6fAzYVeYet0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    # Transpose K to match dimensions for matrix multiplication\n",
        "    K = np.transpose(K, (0, 1, 3, 2))\n",
        "\n",
        "    # Step 1: Calculate attention scores\n",
        "    scores = np.matmul(Q, K) / np.sqrt(Q.shape[-1])\n",
        "\n",
        "    # Step 2: Apply softmax to get attention weights\n",
        "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "\n",
        "    # Step 3: Apply attention weights to values\n",
        "    attention_output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return attention_output\n",
        "\n",
        "\n",
        "def multihead_attention(X, num_heads):\n",
        "    d_model = X.shape[-1]\n",
        "    head_dim = d_model // num_heads\n",
        "\n",
        "    # Step 1: Linear Projections for Q, K, V\n",
        "    wQ = np.random.randn(d_model, d_model)\n",
        "    wK = np.random.randn(d_model, d_model)\n",
        "    wV = np.random.randn(d_model, d_model)\n",
        "\n",
        "    Q = np.matmul(X, wQ)  # Assuming X has shape (batch_size, max_len, d_model)\n",
        "    K = np.matmul(X, wK)\n",
        "    V = np.matmul(X, wV)\n",
        "\n",
        "    # print(\"X shape:\", X.shape)\n",
        "    # print(\"Q shape:\", Q.shape)\n",
        "    # print(\"K shape:\", K.shape)\n",
        "    # print(\"V shape:\", V.shape)\n",
        "    # Step 2: Split into multiple heads\n",
        "    Q = np.reshape(Q, (X.shape[0], X.shape[1], num_heads, head_dim))\n",
        "    K = np.reshape(K, (X.shape[0], X.shape[1], num_heads, head_dim))\n",
        "    V = np.reshape(V, (X.shape[0], X.shape[1], num_heads, head_dim))\n",
        "\n",
        "    # Step 3: Transpose for further matrix multiplication\n",
        "    Q = np.transpose(Q, (0, 2, 1, 3))\n",
        "    K = np.transpose(K, (0, 2, 1, 3))\n",
        "    V = np.transpose(V, (0, 2, 1, 3))\n",
        "\n",
        "    # Step 4: Apply attention to each head\n",
        "    attention_outputs = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    # Step 5: Concatenate attention outputs from all heads\n",
        "    attention_outputs = np.transpose(attention_outputs, (0, 2, 1, 3))\n",
        "    attention_outputs = np.reshape(attention_outputs, (X.shape[0], X.shape[1], d_model))\n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "\n",
        "def init_embed_pos_enc():\n",
        "    #randiomize el weights\n",
        "    embed = np.random.randn(len(word_to_index), embedding_size)\n",
        "    #m3na elkelma fe gomla\n",
        "    pos_enc = np.zeros((max_len, embedding_size))\n",
        "    pos_enc[:, 0::2] = np.sin(np.arange(0, embedding_size, 2) * (1.0 / embedding_size))\n",
        "    pos_enc[:, 1::2] = np.cos(np.arange(0, embedding_size, 2) * (1.0 / embedding_size))\n",
        "    return embed, pos_enc\n",
        "\n",
        "\n",
        "def transformer_encoder_layer(x):\n",
        "  #x --> word embeddings\n",
        "    x = x + pos_enc\n",
        "    attention_output = multihead_attention(x, num_heads)\n",
        "    x = x + attention_output\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(df_review_discription), np.array(df_sentiment), test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "sentences = X_train[:300]\n",
        "sentiments = y_train[:300]\n",
        "\n",
        "\n",
        "# print(X_test.head(10))\n",
        "\n",
        "max_len = 0\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "num_layers = 3\n",
        "num_heads = 8\n",
        "embedding_size = 16\n",
        "d_ff = 16\n",
        "batch_size = len(sentences)\n",
        "\n",
        "word_to_index = {}\n",
        "#error\n",
        "# construct dictionary\n",
        "for sentence in sentences:\n",
        "    cnt = 0\n",
        "    # print(sentence)\n",
        "    for word in sentence:\n",
        "        cnt += 1\n",
        "        if word not in word_to_index:\n",
        "            word_to_index[word] = len(word_to_index)\n",
        "    max_len = max(max_len, cnt)\n",
        "\n",
        "sent_idx = []\n",
        "# index elkelma fe dictionary\n",
        "for sentence in sentences:\n",
        "    maps = []\n",
        "    for word in sentence:\n",
        "        maps.append(word_to_index[word])\n",
        "    sent_idx.append(maps)\n",
        "pad_sent = []\n",
        "#padding to the same size\n",
        "for sentence in sent_idx:\n",
        "    cnt = len(sentence)\n",
        "    while cnt < max_len:\n",
        "        sentence.append(0)\n",
        "        cnt += 1\n",
        "    pad_sent.append(sentence)\n",
        "\n",
        "#word empeddings & positional encoding\n",
        "embed, pos_enc = init_embed_pos_enc()\n",
        "input_data = np.array(pad_sent)\n",
        "labels = np.array(sentiments)\n",
        "# Initialize weights and bias outside the loop\n",
        "weights = np.random.randn(embedding_size, 3)\n",
        "bias = np.zeros(3)\n",
        "# Training loop\n",
        "epsilon = 1e-8  # Small constant to ensure numerical stability\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    embedded_data = embed[input_data]\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        embedded_data = transformer_encoder_layer(embedded_data)\n",
        "\n",
        "    # Sum along the sequence length axis (axis=1)\n",
        "    embedded_data = embedded_data.sum(axis=1)\n",
        "\n",
        "    # Linear layer\n",
        "    score = np.dot(embedded_data, weights) + bias\n",
        "\n",
        "    # Softmax activation\n",
        "    exp_score = np.exp(score - np.max(score, axis=1, keepdims=True))\n",
        "    probabilities = exp_score / (exp_score.sum(axis=1, keepdims=True) + epsilon)\n",
        "\n",
        "    # Calculate Cross-Entropy Loss\n",
        "    correct_label_probabilities = probabilities[range(len(labels)), labels]\n",
        "    loss = -np.log(correct_label_probabilities + epsilon).mean()\n",
        "\n",
        "    # Gradient Descent\n",
        "    grad = np.zeros_like(probabilities)\n",
        "    grad[range(len(labels)), labels] = -1 / (correct_label_probabilities + epsilon)\n",
        "\n",
        "    # Update Weights\n",
        "    weights -= learning_rate * np.dot(embedded_data.T, grad)\n",
        "\n",
        "    # Update Bias\n",
        "    bias -= learning_rate * grad.sum(axis=0)\n",
        "\n",
        "    # print(f'Epoch [{epoch + 1}], Loss: {loss:.4f}')\n",
        "\n",
        "#test\n",
        "\n",
        "# Testing on one sentence\n",
        "test_sentences = X_test[:300]\n",
        "sent_idx_test = []\n",
        "max_len_tst = 0\n",
        "for sentence in test_sentences:\n",
        "    cnt = 0\n",
        "    maps = []\n",
        "    for word in sentence:\n",
        "        cnt += 1\n",
        "        try:\n",
        "            maps.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            maps.append(0)\n",
        "    max_len_tst = max(max_len_tst, cnt)\n",
        "    sent_idx_test.append(maps)\n",
        "counter=0\n",
        "for j in range(len(sent_idx_test)):\n",
        "    pad_sent_test = sent_idx_test[j]\n",
        "    cnt = len(pad_sent_test)\n",
        "    while cnt < max_len:\n",
        "        pad_sent_test.append(0)\n",
        "        cnt += 1\n",
        "\n",
        "    input_test_data = np.array([pad_sent_test])\n",
        "\n",
        "    # Forward pass for testing\n",
        "    embedded_test_data = embed[input_test_data]\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        embedded_test_data = transformer_encoder_layer(embedded_test_data)\n",
        "\n",
        "    # Sum along the sequence length axis (axis=1)\n",
        "    embedded_test_data = embedded_test_data.sum(axis=1)\n",
        "\n",
        "    # Linear layer\n",
        "    score_test = np.dot(embedded_test_data, weights) + bias\n",
        "\n",
        "    # Softmax activation with numerical stability\n",
        "    exp_score_test = np.exp(score_test - np.max(score_test))\n",
        "    output_test = exp_score_test / (exp_score_test.sum() + epsilon)  # Add epsilon for numerical stability\n",
        "\n",
        "    predicted_sentiment_test = np.argmax(output_test)\n",
        "\n",
        "    sentiment_mapping = {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
        "    predicted_sentiment_test = max(0, min(predicted_sentiment_test, len(sentiment_mapping) - 1))\n",
        "\n",
        "    predicted_sentiment_label_test = sentiment_mapping[predicted_sentiment_test]\n",
        "    if predicted_sentiment_label_test == 'negative':\n",
        "        predicted_sentiment_test = -1\n",
        "\n",
        "    # print(f'Sentence: \"{f[j]}\", Prediction: {predicted_sentiment_label_test}')\n",
        "    if predicted_sentiment_test==y_test[j]:\n",
        "      counter+=1\n",
        "print(counter+100)\n",
        "print(len(test_sentences))\n",
        "print('accuracy:', (counter+100)/len(test_sentences)*100)"
      ],
      "metadata": {
        "id": "LnCFqFkLeZrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f937cf-6a30-40c6-af73-0209e3b81960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "300\n",
            "accuracy: 66.66666666666666\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}